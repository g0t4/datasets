{
  "response_message":{
    "finish_reason":"stop",
    "content":"BaseChatModel",
    "reasoning_content":"We need return type hint. Likely returns a model object type, could be Union[ChatAnthropic, ChatOpenAI] but import lazily. Use Any. Probably BaseChatModel. Use from langchain_core.language_models import BaseChatModel. Return BaseChatModel.",
    "role":"assistant",
    "_logged":true
  },
  "request_body":{
    "top_p":1,
    "temperature":1,
    "options":[],
    "messages":[{
        "role":"developer",
        "_logged":true,
        "content":"You are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position: <|fim_middle|>\n\nThe surrounding code is limited, it may not be the full file. Focus on the code near <|fim_middle|>\nDo NOT explain your decisions. Do NOT return markdown blocks ```\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace. Especially on the cursor line!\nYOU ARE ONLY INSERTING CODE. DO NOT REPEAT PREFIX. DO NOT REPEAT SUFFIX.\nComplete as little or as much code as is necessary to help the user. That means part of a line, a full line or even multiple lines! Goal is to help the user as much as is possible and reasonable.\n\nHere are a few examples of tricky completions:\n\n## Example: suggest middle of line (cursor has existing code before and after it)\n\n```python\ndef area(width, height):\n    return <|fim_middle|> * height\n```\n\nSay this is your desired result:\n```python\ndef area(width, height):\n    return width * height\n```\n\n1. then the CORRECT completion is (do not include backticks, those are just to delimit included whitespace):\n`width`\n\n2. WRONG (because repeats the suffix):\n`width * height`\nresults in:\n`    return width * height * height`\n\n3. WRONG (because repeats both suffix and prefix):\n`    return width * height`\nresults in:\n`    return     return width * height * height`\n\n\n## Example: cursor is indented\n\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    <|fim_middle|>\n    end\nend\n```\n\n*Keep in mind, the indent will be there before the suggested code too.*\n\nSay this is your desired result:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n1. then the CORRECT completion is:\n```lua\nelse\n        print(\"Non‑positive\")\n```\n\n2. WRONG (`else` ends up double indented when it should be single):\n```lua\n    else\n        print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n        else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n3. WRONG (`print` ends up single indented when it should be double):\n```lua\nelse\n    print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n    print(\"Non‑positive\")\n    end\nend\n```\n\n\n"
      },{
        "role":"user",
        "_logged":true,
        "content":"Here is context that's automatically provided, that MAY be relevant.\nrepo: dotfiles\n\n## General project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n\n## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n\n## Python Code Preferences\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer `list[]` over `List[]` to mimimize imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n\n\n## Recent yanks (copy to clipboard):\n```.venv/lib/python3.12/site-packages/langchain_anthropic/chat_models.py\nBaseChatModel\n```\n\n"
      },{
        "role":"user",
        "_logged":true,
        "content":"# Semantic Grep matches: 5\n\nThis is automatic context based on my request. These may or may not be relevant.\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/services.py:67-85\ndef use_openai(model: Optional[str] = None):\n\n    # *** gpt 4:\n    # openai https://platform.openai.com/docs/models\n    # model = \"gpt-4o\"  # thru Oct 2023 # currently evaluating\n    # model = \"gpt-4-turbo\" # currently => \"gpt-4-turbo-2024-04-09\"\n    # model = \"gpt-4-turbo-preview\"  # currently => \"gpt-4-0125-preview\" (last used before gpt-4o)\n    # model = \"gpt-4-1106-preview\", # thru Apr 2023 - aka gpt4 turbo # *** first model I used and it works great (used late 2023/early 2024)\n    #\n    # *** gpt 3.5:\n    # model=\"gpt-3.5-turbo-1106\",\n    # gpt-4 \"turbo\" and gpt-3.5-turbo are both fast, so use gpt-4 for accuracy (else 3.5 might need to be re-run/fixed which can cost more)\n\n    return Service(\n        name='openai',\n        api_key=get_api_key('openai', 'ask'),\n        base_url=\"https://api.openai.com/v1\",\n        model=model if model else 'gpt-5',\n    )\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/services.py:24-32\ndef use_vllm(model: Optional[str] = None):\n    return Service(\n        name='vllm',\n        api_key=\"none\",\n        base_url='http://ollama:8000/v1',\n        model=model if model else \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n        # vllm https://github.com/vllm-project/vllm/blob/main/docs/api.md\n        # https://github.com/vllm-project/vllm/blob/main/docs/api.md#chat-completions\n    )\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/services.py:213-232\n\n\ndef get_selected_service_for_args(ask_service, model) -> Service:\n    if ask_service is None:\n        return use_build21(None)\n    # PRN this parsing logic can be shared with single.py, after I re-hydrate the ask_service variable\n    if \"--ollama\" in ask_service:\n        return use_ollama(model)\n    if \"--groq\" in ask_service:\n        return use_groq(model)\n    if \"--deepseek\" in ask_service:\n        return use_deepseek(model)\n    if \"--vllm\" in ask_service:\n        return use_vllm(model)\n    if \"--lmstudio\" in ask_service:\n        return use_lmstudio(model)\n    if \"--anthropic\" in ask_service:\n        return use_anthropic(model)\n    if \"--gh-copilot\" in ask_service:\n        return use_gh_copilot(model)\n\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/services.py:87-94\ndef use_anthropic(model: Optional[str] = None):\n    return Service(\n        name='anthropic',\n        api_key=get_api_key('anthropic', 'ask'),\n        base_url=\"https://api.anthropic.com/v1\",\n        model=model if model else 'claude-sonnet-4-6',\n    )\n    # https://docs.anthropic.com/en/docs/about-claude/models\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/services.py:43-50\ndef use_inception(model: Optional[str] = None):\n    return Service(\n        name='inception',\n        api_key=get_api_key('inception', 'ask'),\n        base_url='https://api.inceptionlabs.ai/v1',\n        model=model if model else 'mercury-coder',\n        # https://platform.inceptionlabs.ai/dashboard/\n    )\n"
      },{
        "role":"user",
        "_logged":true,
        "content":"Please suggest text to replace <|fim_middle|>:\n\n```iterm2/scripts/AutoLaunch/wes/wes/chat_stream.py\nfrom collections.abc import Awaitable, Callable\nimport re\nfrom services import get_selected_service\nfrom logs import log\n\ndef get_model() -> <|fim_middle|>:\n\n    service = get_selected_service()\n    log(f\"using: {service}\")\n\n    if service.name == \"anthropic\":\n        # TODO is this one slow too? measure it\n        from langchain_anthropic import ChatAnthropic\n        model = ChatAnthropic(\n            model_name=service.model,\n            api_key=service.api_key,\n            timeout=None,\n            stop=None,\n        )\n    else:\n        # TODO why is this import so slow?\n        from langchain_openai import ChatOpenAI\n        model = ChatOpenAI(\n            model=service.model,\n            api_key=service.api_key,\n            base_url=service.base_url,\n        )\n\nasync def ask_openai_async_type_response(messages: list[dict], on_chunk: Callable[[str], Awaitable[None]]):\n    log(f\"{messages=}\")\n    model = get_model()\n\n    # max_tokens=use.max_tokens or 200,\n    # TODO temperature? and other model params on Service? (maybe rename it to be ServiceModel combo?)\n    chunks = model.stream(messages)\n    first_chunk = True\n    for chunk in chunks:\n        try:\n            # FYI no reasoning content for llama-server, s/b fine as I don't use that => if I want it, I can add in my ChatLlamaServer impl\n            # strip new lines to avoid submitting commands prematurely, is there an alternate char I could use to split the lines still w/o submitting (would have to check what the shell supports, if anything is possible)... one downside to not being a part of the line editor.. unless there is a workaround? not that I care much b/c multi line commands are not often necessary...\n            sanitized = chunk.content.replace(\"\\n\", \" \")  # PRN check if str before calling replace (i.e. can be list[str] or list[dict]... when is that the case and do I ever use it?)\n            if sanitized.strip() == \"\":\n                continue\n\n            if first_chunk:\n                log(f\"first_chunk: {sanitized}\")\n                sanitized = re.sub(r'```', '', sanitized).lstrip()\n                log(f\"sanitized: {sanitized}\")\n                log(f\"sanitized hex: {sanitized.encode('utf-8').hex()}\")\n                first_chunk = sanitized == \"\"  # stay in \"first_chunk\" mode until first non-empty chunk\n                await session.async_send_text(sanitized)\n            else:\n                await session.async_send_text(sanitized)\n\n            # TODO can I check finish_reason to see if it ran out of tokens?\n            # if choice0.finish_reason == \"length\":\n            #     await session.async_send_text(f\"ran out off tokens, increase max_tokens...\")\n            #     break\n\n        except Exception as e:\n            log(f\"Error processing chunk: {e}\\n chunk: {chunk}\")\n            await session.async_send_text(f\"Error processing chunk: {e}\")\n            return\n```\n\n## Reminders about the cursor line:\nText before cursor: `def get_model() -> `\nText after cursor: `:`"
      }],
    "chat_template_kwargs":{
      "reasoning_effort":"low"
    },
    "raw":false,
    "max_tokens":4096,
    "model":"gpt-oss:120b",
    "stream":true
  },
  "last_sse":{
    "timings":{
      "prompt_n":1118,
      "prompt_ms":163.66,
      "prompt_per_token_ms":0.14638640429338,
      "predicted_ms":285.506,
      "predicted_per_token_ms":4.137768115942,
      "predicted_per_second":241.67618193663,
      "predicted_n":69,
      "prompt_per_second":6831.2354882073,
      "cache_n":1419
    },
    "created":1772091524,
    "object":"chat.completion.chunk",
    "model":"ggml-org/gpt-oss-120b-GGUF",
    "system_fingerprint":"b8115-77d6ae4ac",
    "id":"chatcmpl-eUymnzg5evW4GW4o8EWBHrYcP2Jnw9PK",
    "choices":[{
        "index":0,
        "finish_reason":"stop",
        "delta":[]
      }]
  }
}