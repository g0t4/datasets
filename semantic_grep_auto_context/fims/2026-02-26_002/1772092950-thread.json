{
  "last_sse":{
    "object":"chat.completion.chunk",
    "model":"ggml-org/gpt-oss-120b-GGUF",
    "id":"chatcmpl-pUthXBwJzTECsEU9dnWf0StIbB7gk0DQ",
    "choices":[{
        "delta":[],
        "finish_reason":"stop",
        "index":0
      }],
    "created":1772092951,
    "system_fingerprint":"b8115-77d6ae4ac",
    "timings":{
      "predicted_ms":160.581,
      "predicted_per_token_ms":4.2258157894737,
      "predicted_per_second":236.6406984637,
      "predicted_n":38,
      "prompt_per_second":7975.36363498,
      "prompt_per_token_ms":0.12538613231552,
      "prompt_ms":197.107,
      "cache_n":1644,
      "prompt_n":1572
    }
  },
  "response_message":{
    "finish_reason":"stop",
    "reasoning_content":"We need to fill max_tokens argument. Likely use service.max_tokens or default. So insert `service.max_tokens` .",
    "content":"service.max_tokens",
    "_logged":true,
    "role":"assistant"
  },
  "request_body":{
    "temperature":1,
    "chat_template_kwargs":{
      "reasoning_effort":"low"
    },
    "options":[],
    "raw":false,
    "model":"gpt-oss:120b",
    "max_tokens":4096,
    "top_p":1,
    "stream":true,
    "messages":[{
        "_logged":true,
        "role":"developer",
        "content":"You are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position: <|fim_middle|>\n\nThe surrounding code is limited, it may not be the full file. Focus on the code near <|fim_middle|>\nDo NOT explain your decisions. Do NOT return markdown blocks ```\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace. Especially on the cursor line!\nYOU ARE ONLY INSERTING CODE. DO NOT REPEAT PREFIX. DO NOT REPEAT SUFFIX.\nComplete as little or as much code as is necessary to help the user. That means part of a line, a full line or even multiple lines! Goal is to help the user as much as is possible and reasonable.\n\nHere are a few examples of tricky completions:\n\n## Example: suggest middle of line (cursor has existing code before and after it)\n\n```python\ndef area(width, height):\n    return <|fim_middle|> * height\n```\n\nSay this is your desired result:\n```python\ndef area(width, height):\n    return width * height\n```\n\n1. then the CORRECT completion is (do not include backticks, those are just to delimit included whitespace):\n`width`\n\n2. WRONG (because repeats the suffix):\n`width * height`\nresults in:\n`    return width * height * height`\n\n3. WRONG (because repeats both suffix and prefix):\n`    return width * height`\nresults in:\n`    return     return width * height * height`\n\n\n## Example: cursor is indented\n\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    <|fim_middle|>\n    end\nend\n```\n\n*Keep in mind, the indent will be there before the suggested code too.*\n\nSay this is your desired result:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n1. then the CORRECT completion is:\n```lua\nelse\n        print(\"Non‑positive\")\n```\n\n2. WRONG (`else` ends up double indented when it should be single):\n```lua\n    else\n        print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n        else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n3. WRONG (`print` ends up single indented when it should be double):\n```lua\nelse\n    print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n    print(\"Non‑positive\")\n    end\nend\n```\n\n\n"
      },{
        "_logged":true,
        "role":"user",
        "content":"Here is context that's automatically provided, that MAY be relevant.\nrepo: dotfiles\n\n## General project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n\n## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n\n## Python Code Preferences\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer `list[]` over `List[]` to mimimize imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n\n\n## Recent yanks (copy to clipboard):\n```iterm2/scripts/AutoLaunch/wes/wes/chat_stream.py\n\ndef get_model() -> tuple[BaseChatModel, Service]:\n    service = get_selected_service()\n    log(f\"using: {service}\")\n\n    if service.name == \"anthropic\":\n        # TODO is this one slow too? measure it\n        from langchain_anthropic import ChatAnthropic\n        model = ChatAnthropic(\n            model_name=service.model,\n            api_key=service.api_key,\n            timeout=None,\n            stop=None,\n        )\n        return model, service\n\n    # TODO why is this import so slow?\n    from langchain_openai import ChatOpenAI\n    model = ChatOpenAI(\n        model=service.model,\n        api_key=service.api_key,\n        base_url=service.base_url,\n    )\n    return model, service\n```\n\n"
      },{
        "_logged":true,
        "role":"user",
        "content":"# Semantic Grep matches: 5\n\nThis is automatic context based on my request. These may or may not be relevant.\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/chat_non_stream.py:8-52\ndef generate_non_streaming(passed_context: str, system_message: str, max_tokens: int):\n    messages = [\n        SystemMessage(content=system_message),\n        HumanMessage(content=passed_context),\n    ]\n\n    service = get_selected_service()\n    if service.name == \"anthropic\":\n        from langchain_anthropic import ChatAnthropic\n        model = ChatAnthropic(\n            model_name=service.model,\n            api_key=service.api_key,\n            timeout=None,\n            stop=None,\n        )\n    else:\n        from langchain_openai import ChatOpenAI\n        model = ChatOpenAI(\n            model=service.model,\n            api_key=service.api_key,\n            base_url=service.base_url,\n        )\n\n    # TODO add temperature (optional) to Service? (perhaps rename to ServiceModel or split out generation args somehow)\n    # PRN timeout?\n    ai_message = model.invoke(messages, max_tokens=max_tokens)\n    content = ai_message.content\n    if not isinstance(content, str):\n        # FYI implement other types as encountered so I can see what I am working with\n        return f\"ABORT... expected string content but got {type(content).__name__} for response: {content}\"\n\n    try:\n        # FYI no reasoning content for llama-server, s/b fine as I don't use that => if I want it, I can add in my ChatLlamaServer impl\n        # strip new lines to avoid submitting commands prematurely, is there an alternate char I could use to split the lines still w/o submitting (would have to check what the shell supports, if anything is possible)... one downside to not being a part of the line editor.. unless there is a workaround? not that I care much b/c multi line commands are not often necessary...\n        sanitized = content.replace(\"\\n\", \" \")  # PRN check if str before calling replace (i.e. can be list[str] or list[dict]... when is that the case and do I ever use it?)\n        sanitized = re.sub(r'```', '', sanitized).lstrip()\n\n        # TODO should I check finish_reason to see if it ran out of tokens? and warn if so? (or better yet, offer to resume?)\n        # if choice0.finish_reason == \"length\":\n        #     await session.async_send_text(f\"ran out off tokens, increase max_tokens...\")\n        #     break\n\n        return sanitized\n    except Exception as e:\n        return f\"Error processing chunk: {e}\\n chunk: {content}\"\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/chat_non_stream.py:0-19\nimport re\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom services import Service, get_selected_service\n\n# TODO testing this with only one consumer: (ctrl-b via single.py)\n# TODO test anthropic\n# DONE: openai/llama-server\n\ndef generate_non_streaming(passed_context: str, system_message: str, max_tokens: int):\n    messages = [\n        SystemMessage(content=system_message),\n        HumanMessage(content=passed_context),\n    ]\n\n    service = get_selected_service()\n    if service.name == \"anthropic\":\n        from langchain_anthropic import ChatAnthropic\n        model = ChatAnthropic(\n            model_name=service.model,\n            api_key=service.api_key,\n\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/chat_non_stream.py:15-34\n    if service.name == \"anthropic\":\n        from langchain_anthropic import ChatAnthropic\n        model = ChatAnthropic(\n            model_name=service.model,\n            api_key=service.api_key,\n            timeout=None,\n            stop=None,\n        )\n    else:\n        from langchain_openai import ChatOpenAI\n        model = ChatOpenAI(\n            model=service.model,\n            api_key=service.api_key,\n            base_url=service.base_url,\n        )\n\n    # TODO add temperature (optional) to Service? (perhaps rename to ServiceModel or split out generation args somehow)\n    # PRN timeout?\n    ai_message = model.invoke(messages, max_tokens=max_tokens)\n    content = ai_message.content\n\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/chat_non_stream.py:30-49\n\n    # TODO add temperature (optional) to Service? (perhaps rename to ServiceModel or split out generation args somehow)\n    # PRN timeout?\n    ai_message = model.invoke(messages, max_tokens=max_tokens)\n    content = ai_message.content\n    if not isinstance(content, str):\n        # FYI implement other types as encountered so I can see what I am working with\n        return f\"ABORT... expected string content but got {type(content).__name__} for response: {content}\"\n\n    try:\n        # FYI no reasoning content for llama-server, s/b fine as I don't use that => if I want it, I can add in my ChatLlamaServer impl\n        # strip new lines to avoid submitting commands prematurely, is there an alternate char I could use to split the lines still w/o submitting (would have to check what the shell supports, if anything is possible)... one downside to not being a part of the line editor.. unless there is a workaround? not that I care much b/c multi line commands are not often necessary...\n        sanitized = content.replace(\"\\n\", \" \")  # PRN check if str before calling replace (i.e. can be list[str] or list[dict]... when is that the case and do I ever use it?)\n        sanitized = re.sub(r'```', '', sanitized).lstrip()\n\n        # TODO should I check finish_reason to see if it ran out of tokens? and warn if so? (or better yet, offer to resume?)\n        # if choice0.finish_reason == \"length\":\n        #     await session.async_send_text(f\"ran out off tokens, increase max_tokens...\")\n        #     break\n\n\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/services.py:10-22\nclass Service(NamedTuple):\n    base_url: str\n    model: str\n    api_key: str\n    name: str\n    max_tokens: int | None = None\n\n    def __repr__(self):\n        # !!! DO NOT INCLUDE api_key\n        attrs = f\"{self.name} model={self.model} chat_url={self.base_url}\"\n        if self.max_tokens:\n            attrs += f\" max_tokens={self.max_tokens}\"\n        return f\"Service({attrs})\"\n"
      },{
        "_logged":true,
        "role":"user",
        "content":"Please suggest text to replace <|fim_middle|>:\n\n```iterm2/scripts/AutoLaunch/wes/wes/chat_stream.py\nimport re\nfrom collections.abc import Awaitable, Callable\nfrom services import get_selected_service\nfrom logs import log\nfrom langchain_core.language_models import BaseChatModel\n\ndef get_model() -> tuple[BaseChatModel, Service]:\n    service = get_selected_service()\n    log(f\"using: {service}\")\n\n    if service.name == \"anthropic\":\n        # TODO is this one slow too? measure it\n        from langchain_anthropic import ChatAnthropic\n        model = ChatAnthropic(\n            model_name=service.model,\n            api_key=service.api_key,\n            timeout=None,\n            stop=None,\n        )\n        return model, service\n\n    # TODO why is this import so slow?\n    from langchain_openai import ChatOpenAI\n    model = ChatOpenAI(\n        model=service.model,\n        api_key=service.api_key,\n        base_url=service.base_url,\n    )\n    return model, service\n\nasync def ask_openai_async_type_response(messages: list[dict], on_chunk: Callable[[str], Awaitable[None]]):\n    log(f\"{messages=}\")\n    model, service = get_model()\n\n    # max_tokens=use.max_tokens or 200,\n    # TODO temperature? and other model params on Service? (maybe rename it to be ServiceModel combo?)\n    chunks = model.stream(messages, max_tokens=<|fim_middle|>)\n    first_chunk = True\n    for chunk in chunks:\n        try:\n            # FYI no reasoning content for llama-server, s/b fine as I don't use that => if I want it, I can add in my ChatLlamaServer impl\n            # strip new lines to avoid submitting commands prematurely, is there an alternate char I could use to split the lines still w/o submitting (would have to check what the shell supports, if anything is possible)... one downside to not being a part of the line editor.. unless there is a workaround? not that I care much b/c multi line commands are not often necessary...\n            sanitized = chunk.content.replace(\"\\n\", \" \")  # PRN check if str before calling replace (i.e. can be list[str] or list[dict]... when is that the case and do I ever use it?)\n            if sanitized.strip() == \"\":\n                continue\n\n            if first_chunk:\n                log(f\"first_chunk: {sanitized}\")\n                sanitized = re.sub(r'```', '', sanitized).lstrip()\n                log(f\"sanitized: {sanitized}\")\n                log(f\"sanitized hex: {sanitized.encode('utf-8').hex()}\")\n                first_chunk = sanitized == \"\"  # stay in \"first_chunk\" mode until first non-empty chunk\n\n            await on_chunk(sanitized)\n\n            # TODO can I check finish_reason to see if it ran out of tokens?\n            # if choice0.finish_reason == \"length\":\n            #     await on_chunk(f\"ran out off tokens, increase max_tokens...\")\n            #     break\n\n        except Exception as e:\n            log(f\"Error processing chunk: {e}\\n chunk: {chunk}\")\n            await on_chunk(f\"Error processing chunk: {e}\")\n            return\n```\n\n## Reminders about the cursor line:\nText before cursor: `    chunks = model.stream(messages, max_tokens=`\nText after cursor: `)`"
      }]
  }
}