{
  "last_sse":{
    "choices":[{
        "finish_reason":"stop",
        "delta":[],
        "index":0
      }],
    "object":"chat.completion.chunk",
    "system_fingerprint":"b8115-77d6ae4ac",
    "timings":{
      "cache_n":0,
      "prompt_n":4105,
      "predicted_per_second":208.63136674983,
      "prompt_per_second":8690.7183974358,
      "prompt_per_token_ms":0.1150652862363,
      "predicted_per_token_ms":4.7931431192661,
      "prompt_ms":472.343,
      "predicted_n":545,
      "predicted_ms":2612.263
    },
    "model":"ggml-org/gpt-oss-120b-GGUF",
    "id":"chatcmpl-ueiQYDZKgxooGdxknHkwHmJ0ZEQpwxgg",
    "created":1772351263
  },
  "response_message":{
    "_logged":true,
    "finish_reason":"stop",
    "content":"--- @type fun(client_id: number): boolean\nlocal function is_ask_language_server(client_id)\n    local client = vim.lsp.get_client_by_id(client_id)\n    return client ~= nil and client.name == \"ask_language_server\"\nend\n\n--- @alias EventArgs { id:number, event: string, group: number|nil, file: string, match: string, buf:number, data: table }\nvim.api.nvim_create_autocmd('LspAttach', {\n    callback = --- @param event_args EventArgs\n        function(event_args)\n            log:info(string.format(\"LspAttach: client_id=%s (buf %d)\", event_args.data.client_id, event_args.buf))\n            if not is_ask_language_server(event_args.data.client_id) then return end\n        end\n})\n\n-- if LS provides dynamic registration, then some capabilities aren't registered until after LspAttach event\nvim.lsp.handlers['client/registerCapability'] = (function(overridden)\n    -- see :h LspAttach\n    return function(err, res, ctx)\n        log:info(string.format(\"client/registerCapability: client_id=%s\", ctx.client_id))\n        local result = overridden(err, res, ctx)\n        if not is_ask_language_server(ctx.client_id) then return result end\n\n        local client = vim.lsp.get_client_by_id(ctx.client_id)\n        for bufnr, _ in pairs(client.attached_buffers) do\n            -- Call your custom on_attach logic...\n            -- my_on_attach(client, bufnr)\n        end\n        return result\n    end\nend)(vim.lsp.handlers['client/registerCapability'])\n\nvim.api.nvim_create_autocmd('LspDetach', {\n    callback = --- @param event_args EventArgs\n        function(event_args)\n            log:info(string.format(\"LspDetach: client_id=%s (buf %d)\", event_args.data.client_id, event_args.buf))\n            if not is_ask_language_server(event_args.data.client_id) then return end\n\n            local client = vim.lsp.get_client_by_id(event_args.data.client_id)\n            if client:supports_method('textDocument/formatting') then\n                vim.api.nvim_clear_autocmds({\n                    event = 'BufWritePre',\n                    buffer = event_args.buf,\n                })\n            end\n        end,\n})",
    "reasoning_content":"We need to extract helper function is_ask_language_server(client_id) and use it in each handler. Add function with annotation. Modify three places: LspAttach callback, client/registerCapability handler, LspDetach callback. Use the helper. Provide code only.",
    "role":"assistant"
  },
  "request_body":{
    "chat_template_kwargs":{
      "reasoning_effort":"low"
    },
    "top_p":1,
    "stream":true,
    "temperature":1,
    "messages":[{
        "content":"## Ground rules:\n- Follow the user's instructions. \n- DO NOT wrap answers in markdown code blocks, which means no triple backticks like ``` nor single backtick like ` .\n- Do not explain answers, just give me code. \n- If changing existing code, preserve unrelated code and comments. \n- Never add comments to the end of a line. \n- Never add stupid comments.\n- Be considerate with indentation. \n- Prefer readable code over of comments. \n- Prefer meaningful names for variables, functions, etc. Avoid ambiguous names.",
        "_logged":true,
        "role":"system"
      },{
        "content":"## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n",
        "_logged":true,
        "role":"user"
      },{
        "content":"## Lua Code Preferences\n\nNaming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\nType Hints\n- Prefer EmmyLua/LuaLS style annotations\n- Always annotate when type inference would be ambiguous, or for public APIs.\n\nExamples:\n```lua\n---@param names string[]\n---@param checker fun(name: string): boolean\n---@return table<string, integer>\nfunction process_names(names, checker) end\n\n---@class WindowController\n---@field window hs.axuielement\n---@field title? string\n---@field size { w: number, h: number }\nlocal WindowController = {}\n```\n\n",
        "_logged":true,
        "role":"user"
      },{
        "content":"# Semantic Grep matches: 5\n\nThis is automatic context based on my request. These may or may not be relevant.\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag/client/init.lua:134-150\nfunction M.is_rag_supported_in_current_file(bufnr)\n    -- TODO add a virtual toggle so LSP failure stops requests too\n    --   can I detect non-connected LSP (w/o hugh perf hit?)\n\n    if not M.is_rag_supported() then\n        return false\n    end\n\n    bufnr = bufnr or 0\n    local buffer_name = vim.api.nvim_buf_get_name(bufnr)\n    local extension = vim.fn.fnamemodify(buffer_name, \":e\")\n    -- TODO use filetype instead?\n    --  i.e. .yaml/.yml\n    --  or c: .h/.c/.cpp ...\n    --  or node: .js/.mjs\n    return vim.tbl_contains(M.rag_indexed_file_extensions, extension)\nend\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/questions/frontend.lua:0-19\nlocal buffers = require(\"ask-openai.helpers.buffers\")\nlocal log = require(\"ask-openai.logs.logger\").predictions()\nlocal completion_logger = require(\"ask-openai.logs.completion_logger\")\nlocal tool_router = require(\"ask-openai.tools.router\")\nlocal curl = require(\"ask-openai.backends.curl\")\nlocal agentica = require(\"ask-openai.backends.models.agentica\")\nlocal ChatWindow = require(\"ask-openai.questions.chat.window\")\nlocal ChatThread = require(\"ask-openai.questions.chat.thread\")\nlocal TxChatMessage = require(\"ask-openai.questions.chat.messages.tx\")\nlocal ChatParams = require(\"ask-openai.questions.chat.params\")\nlocal Selection = require(\"ask-openai.helpers.selection\")\nlocal CurrentContext = require(\"ask-openai.predictions.context\")\nlocal api = require(\"ask-openai.api\")\nlocal rag_client = require(\"ask-openai.rag.client\")\nlocal files = require(\"ask-openai.helpers.files\")\nlocal model_params = require(\"ask-openai.questions.models.params\")\nlocal LinesBuilder = require(\"ask-openai.questions.lines_builder\")\nlocal MessageBuilder = require(\"ask-openai.rewrites.message_builder\")\nlocal prompts = require(\"ask-openai.predictions.context.prompts\")\nlocal HLGroups = require(\"ask-openai.hlgroups\")\n\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rewrites/frontend.lua:0-19\nlocal buffers = require(\"ask-openai.helpers.buffers\")\nlocal curl = require(\"ask-openai.backends.curl\")\nlocal log = require(\"ask-openai.logs.logger\").predictions()\nlocal agentica = require(\"ask-openai.backends.models.agentica\")\nlocal text_helpers = require(\"ask-openai.helpers.text\")\nlocal thinking = require(\"ask-openai.rewrites.thinking\")\nlocal Selection = require(\"ask-openai.helpers.selection\")\nlocal Displayer = require(\"ask-openai.rewrites.displayer\")\nlocal CurrentContext = require(\"ask-openai.predictions.context\")\nlocal TxChatMessage = require(\"ask-openai.questions.chat.messages.tx\")\nlocal files = require(\"ask-openai.helpers.files\")\nlocal api = require(\"ask-openai.api\")\nlocal rag_client = require(\"ask-openai.rag.client\")\nlocal CurlRequest = require(\"ask-openai.backends.curl_request\")\nlocal human = require(\"devtools.humanize\")\nlocal tool_router = require(\"ask-openai.tools.router\")\nlocal model_params = require(\"ask-openai.questions.models.params\")\nlocal MessageBuilder = require(\"ask-openai.rewrites.message_builder\")\nlocal HLGroups = require(\"ask-openai.hlgroups\")\nlocal harmony = require(\"ask-openai.backends.models.gptoss.tokenizer\").harmony\n\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/tools/mcp/init.lua:0-19\nlocal log = require(\"ask-openai.logs.logger\").predictions()\nlocal ansi = require(\"ask-openai.predictions.ansi\")\nlocal plumbing = require(\"ask-openai.tools.plumbing\")\nlocal safely = require(\"ask-openai.helpers.safely\")\n\nlocal uv = vim.uv\n\nlocal M = {}\n\nM.counter = 1\nM.callbacks = {}\n\n-- TODO! look into Memory tools / RAG, i.e. in Qwen-Agent\n-- TODO! https://github.com/QwenLM/Qwen-Agent/blob/main/qwen_agent/memory/memory.py\n-- also check out other examples in qwen-agent for new ideas:\n--    https://github.com/QwenLM/Qwen-Agent/tree/main/examples\n-- PRN also I wanna test out large qwen models, hosted by Alibaba/groq/others\n\nlocal servers = {\n\n\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/questions/frontend.lua:46-261\n---@param opts {args:string}\nlocal function ask_question_command(opts)\n    local user_prompt = opts.args\n    local always_include = {\n        yanks = true,\n        project = true,\n    }\n    local context = CurrentContext:items(user_prompt, always_include)\n    local cleaned_prompt = context.includes.cleaned_prompt\n\n    -- * /selection (currently needs current window to be code window)\n    local selection = nil\n    if context.includes.include_selection then\n        -- FYI include_selection basically captures if user had selection when they first invoked a keymap to submit this command\n        --   b/c submitting command switches modes, also user might unselect text on accident (or want to repeat w/ prev selection)\n        --   thus it is useful to capture intent with /selection early on\n\n        -- FYI my Selection helper only works on current window... so I can't put this off I need it way up high:\n        -- NOT IMPLEMENTED (yet?) local selection = Selection._get_visual_selection_for_window_id(code_win_id)\n        selection = Selection.get_visual_selection_for_current_window()\n        if selection:is_empty() then\n            error(\"No /selection found (no current, nor prior, selection).\")\n            return\n        end\n        -- TODO once I get Selection working w/ non-current windows, move this code down and cleanup/simplify the spot that uses selection below\n    end\n\n    -- FYI! do not move opening window higher, unless above code supports code_win_id/code_bufnr:\n    QuestionsFrontend.ensure_chat_window_is_open()\n    --\n    -- * chat window should always be open, nonetheless check:\n    local buffer_name = vim.api.nvim_buf_get_name(0)\n    local chat_window_is_open = buffer_name:match(\"AskQuestion$\")\n    local code_win_id = vim.api.nvim_get_current_win()\n    local code_bufnr = 0 -- 0 == current\n    if chat_window_is_open then\n        -- * chat window is open, get prior window's code_win_id and code_bufnr\n        code_win_id = vim.fn.win_getid(vim.fn.winnr('#'))\n        code_bufnr = vim.api.nvim_win_get_buf(code_win_id)\n    end\n    -- log:error(\"code_win_id\", code_win_id)\n    -- log:error(\"code_bufnr\", code_bufnr)\n\n    QuestionsFrontend.abort_last_request()\n    use_tools = context.includes.use_tools or false\n\n    local system = get_file(\"~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/questions/prompts/system_message.md\")\n    -- PRN \"NEVER add copyright or license headers unless specifically requested.\"\n\n    local tool_definitions\n    if use_tools then\n        -- PRN build out more detailed guidance: review Claude Code and Codex prompts\n        local tool_instructs = get_file(\"~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/questions/prompts/tools.md\")\n        tool_instructs = tool_instructs:gsub(\"INSERT_CWD\", vim.fn.getcwd())\n        system = system .. \"\\n\\n\" .. tool_instructs\n\n        local tool_provided_instructs\n        tool_definitions, tool_provided_instructs = tool_router.openai_tools()\n        if tool_provided_instructs then\n            system = system .. \"\\n\\n\" .. table.concat(tool_provided_instructs, \"\\n\")\n        end\n\n        -- If readonly mode, remove editing tools like apply_patch\n        if context.includes.readonly then\n            local filtered = {}\n            for _, tool in ipairs(tool_definitions) do\n                local name = tool[\"function\"] and tool[\"function\"].name or nil\n                if name ~= \"apply_patch\" then\n                    table.insert(filtered, tool)\n                end\n            end\n            tool_definitions = filtered\n        end\n    end\n\n    -- * display system message in chat window\n    if not first_turn_ns_id then\n        first_turn_ns_id = vim.api.nvim_create_namespace(\"ask.marks.chat.window.first.turn\")\n    end\n    local lines = LinesBuilder:new(first_turn_ns_id)\n    if QuestionsFrontend.thread then\n        -- FYI some previous extmarks are \"dropped\", fine by me to \"turn off the colors\"... but, probably want it for all previous chat extmarks\n        lines:append_styled_lines({ \"--- New Thread Started ---\" }, HLGroups.SYSTEM_PROMPT)\n        -- or:   QuestionsFrontend.clear_chat_command()\n    end\n    lines:mark_next_line(HLGroups.SYSTEM_PROMPT)\n    lines:append_folded_styled_text(\"system\\n\" .. system, \"\")\n\n\n    -- * display user message in chat window\n    lines:append_role_header(\"user\")\n    lines:append_text(cleaned_prompt)\n\n    local user_message = cleaned_prompt\n    local code_context = nil\n    if selection then\n        local file_name = files.get_file_relative_path(code_bufnr)\n        -- include line range in the filename like foo.py:10-20\n        local start_line = selection:start_line_1indexed()\n        local end_line = selection:end_line_1indexed()\n        local line_info = start_line == end_line and tostring(start_line) or (start_line .. \"-\" .. end_line)\n        local file_display = file_name .. \":\" .. line_info\n        code_context =\n            \"Here is the code I selected:\" .. \"\\n```\" .. file_display .. \"\\n\" .. selection.original_text .. \"\\n```\"\n\n        -- PRN count \\n in selection.original_text and only fold if > 10\n        local fold = false -- = newline_count > 10\n        if fold then\n            lines:append_folded_styled_text(code_context, \"\")\n        else\n            lines:append_styled_text(code_context, \"\")\n        end\n        user_message = user_message .. \"\\n\\n\" .. code_context\n    end\n\n    if context.includes.current_file then\n        local entire_file_message = MessageBuilder:new()\n            :plain_text(\"FYI, here is my current buffer in Neovim. Use this as context for my request:\")\n            :md_current_buffer(code_bufnr)\n            :to_text()\n\n        -- skip code_context if entire file selected (user intent matters, entire file is vague)\n        lines:append_folded_styled_text(entire_file_message, \"\")\n        user_message = user_message .. \"\\n\\n\" .. entire_file_message\n    end\n\n    lines:append_blank_line()\n    QuestionsFrontend.chat_window:append_styled_lines(lines)\n\n    ---@type OpenAIChatCompletion_TxChatMessage[]\n    local messages = {\n        TxChatMessage:system(system)\n    }\n\n    -- ? context.includes.open_files\n    if context.includes.yanks and context.yanks then\n        -- PRN anything I want to show about auto context? (not just yanks)\n        table.insert(messages, TxChatMessage:user_context(context.yanks.content))\n    end\n    if context.includes.commits and context.commits then\n        for _, commit in pairs(context.commits) do\n            table.insert(messages, TxChatMessage:user_context(commit.content))\n        end\n    end\n    if context.includes.project and context.project then\n        -- TODO does any of this belong in the system_message?\n        --  ? actually test if repeating some of this here helps\n        --    i.e. my global project instructions include not touching unrelated code too\n        --  perhaps leave the system_message for coding instructions specific to AskQuestion...?\n        vim.iter(context.project)\n            :each(function(value)\n                table.insert(messages, TxChatMessage:user_context(value.content))\n            end)\n    end\n\n    local function then_generate_completion(rag_matches)\n        local rag_message = prompts.semantic_grep_user_message(rag_matches)\n        if rag_message then\n            table.insert(messages, rag_message)\n        end\n\n        -- * user request should be last\n        -- FYI I had this before RAG matches and it was working fine too\n        table.insert(messages, TxChatMessage:user(user_message))\n\n        local base_url = \"http://build21:8013\"\n        local body_overrides = model_params.new_gptoss_chat_body_llama_server({\n            -- local body_overrides = model_params.new_qwen3coder_llama_server_chat_body({\n            messages = messages,\n            model = \"\", -- irrelevant for llama-server\n            tools = tool_definitions,\n        })\n\n        QuestionsFrontend.thread = ChatThread:new(body_overrides, base_url)\n        -- log:info(\"sending\", vim.inspect(QuestionsFrontend.thread))\n        QuestionsFrontend.then_send_messages()\n    end\n\n    -- log:error(\"context.includes\", vim.inspect(context.includes))\n    if api.is_rag_enabled() and not context.includes.norag and rag_client.is_rag_supported_in_current_file(code_bufnr) then\n        local this_request_ids, cancel -- declare in advance for closure\n\n        ---@param rag_matches LSPRankedMatch[]\n        function on_rag_response(rag_matches)\n            log:info(\"on_rag_response\")\n            -- * make sure prior (canceled) rag request doesn't still respond\n            if QuestionsFrontend.rag_request_ids ~= this_request_ids then\n                log:trace(\"possibly stale rag results, skipping: \" .. vim.inspect({\n                    global_rag_request_ids = QuestionsFrontend.rag_request_ids,\n                    this_request_ids = this_request_ids,\n                }))\n                return\n            end\n\n            if QuestionsFrontend.rag_cancel == nil then\n                log:error(\"rag appears canceled, skipping on_rag_response...\")\n                return\n            end\n\n            then_generate_completion(rag_matches)\n        end\n\n        this_request_ids, cancel = rag_client.context_query_questions(code_bufnr, cleaned_prompt, code_context, context.includes.top_k, on_rag_response)\n        QuestionsFrontend.rag_cancel = function()\n            log:warn(\"canceling RAG\")\n            QuestionsFrontend.rag_cancel = nil\n            cancel()\n            QuestionsFrontend.rag_request_ids = nil\n        end\n        QuestionsFrontend.rag_request_ids = this_request_ids\n    else\n        QuestionsFrontend.rag_cancel = nil\n        QuestionsFrontend.rag_request_ids = nil\n        then_generate_completion({})\n    end\nend\n",
        "_logged":true,
        "role":"user"
      },{
        "content":"extract helper function for is_ask_language_server(client_id) and use that in each handler\n\nHere is the code I selected:\n```lua/ask-openai/rag/init.lua:88-134\n\n    --- @alias EventArgs { id:number, event: string, group: number|nil, file: string, match: string, buf:number, data: table }\n    vim.api.nvim_create_autocmd('LspAttach', {\n        callback =\n        --- @param event_args EventArgs\n            function(event_args)\n                log:info(string.format(\"LspAttach: client_id=%s (buf %d)\", event_args.data.client_id, event_args.buf))\n                local client = vim.lsp.get_client_by_id(event_args.data.client_id)\n                if not client or client.name ~= \"ask_language_server\" then return end\n            end\n    })\n\n    -- if LS provides dynamic registration, then some capabilities aren't registered until after LspAttach event\n    vim.lsp.handlers['client/registerCapability'] = (function(overridden)\n        -- see :h LspAttach\n        return function(err, res, ctx)\n            -- TODO if I don't have this in my LS then comment out this handler\n            log:info(string.format(\"client/registerCapability: client_id=%s\", ctx.client_id))\n            local result = overridden(err, res, ctx)\n            local client = vim.lsp.get_client_by_id(ctx.client_id)\n            if not client or client.name ~= \"ask_language_server\" then return end\n\n            for bufnr, _ in pairs(client.attached_buffers) do\n                -- Call your custom on_attach logic...\n                -- my_on_attach(client, bufnr)\n            end\n            return result\n        end\n    end)(vim.lsp.handlers['client/registerCapability'])\n\n    vim.api.nvim_create_autocmd('LspDetach', {\n        callback =\n        --- @param event_args EventArgs\n            function(event_args)\n                log:info(string.format(\"LspDetach: client_id=%s (buf %d)\", event_args.data.client_id, event_args.buf))\n                local client = vim.lsp.get_client_by_id(event_args.data.client_id)\n                if not client or client.name ~= \"ask_language_server\" then return end\n\n                -- Remove the autocommand to format the buffer on save, if it exists\n                if client:supports_method('textDocument/formatting') then\n                    vim.api.nvim_clear_autocmds({\n                        event = 'BufWritePre',\n                        buffer = event_args.buf,\n                    })\n                end\n            end,\n    })\n```\n\nThis is not necessarily a complete selection of nearby code, this is just the part I want help with. Preserve indentation!",
        "_logged":true,
        "role":"user"
      }],
    "model":"",
    "max_tokens":4096
  }
}