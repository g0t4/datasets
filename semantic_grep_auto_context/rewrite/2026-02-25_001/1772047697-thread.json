{
  "response_message":{
    "finish_reason":"stop",
    "content":"```python\n    if \"--ollama\" in ask_service:\n        return use_ollama(model)\n    if \"--groq\" in ask_service:\n        return use_groq(model)\n    if \"--deepseek\" in ask_service:\n        return use_deepseek(model)\n    if \"--vllm\" in ask_service:\n        return use_vllm(model)\n    if \"--lmstudio\" in ask_service:\n        return use_lmstudio(model)\n    if \"--anthropic\" in ask_service:\n        return use_anthropic(model)\n    if \"--gh-copilot\" in ask_service:\n        return use_gh_copilot(model)\n    if \"--build21\" in ask_service:\n        return use_build21(model)\n    if \"--inception\" in ask_service:\n        return use_inception(model)\n    if \"--xai\" in ask_service:\n        return use_xai(model)\n    if \"--openai\" in ask_service:\n        return use_openai(model)\n```",
    "reasoning_content":"We need to change equality checks to `in` for all cases like \"--build21\". So replace `if ask_service == \"--ollama\":` with `if \"--ollama\" in ask_service:` etc. Provide modified snippet.",
    "role":"assistant",
    "_logged":true
  },
  "request_body":{
    "chat_template_kwargs":{
      "reasoning_effort":"low"
    },
    "model":"",
    "stream":true,
    "messages":[{
        "role":"system",
        "content":"## Ground rules:\n- Follow the user's instructions. \n- DO NOT wrap answers in markdown code blocks, which means no triple backticks like ``` nor single backtick like ` .\n- Do not explain answers, just give me code. \n- If changing existing code, preserve unrelated code and comments. \n- Never add comments to the end of a line. \n- Never add stupid comments.\n- Be considerate with indentation. \n- Prefer readable code over of comments. \n- Prefer meaningful names for variables, functions, etc. Avoid ambiguous names.",
        "_logged":true
      },{
        "role":"user",
        "content":"## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n",
        "_logged":true
      },{
        "role":"user",
        "content":"## Python Code Preferences\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer `list[]` over `List[]` to mimimize imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n",
        "_logged":true
      },{
        "role":"user",
        "content":"# Semantic Grep matches: 5\n\nThis is automatic context based on my request. These may or may not be relevant.\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/suggest.py:0-3\nfrom services import Service\nimport httpx  # import alone is ~58ms\n\n\n\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/suggest.py:4-8\ndef generate(passed_context: str, system_message: str, use: Service, max_tokens: int):\n    if use.name == \"anthropic\":\n        return get_anthropic_suggestion(passed_context, system_message, use, max_tokens)\n    else:\n        return get_openai_suggestion(passed_context, system_message, use, max_tokens)\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/suggest.py:11-45\ndef get_openai_suggestion(passed_context: str, system_message: str, use: Service, max_tokens: int):\n    http_client = httpx.Client()\n    try:\n        body = {\n            \"model\": use.model,\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": system_message\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"{passed_context}\"\n                },\n            ],\n            \"max_tokens\": max_tokens,\n            \"n\": 1  # default\n        }\n        chat_url = use.chat_url()\n        headers = {\n            \"Authorization\": f\"Bearer {use.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        response = http_client.post(chat_url, json=body, headers=headers)\n        response.raise_for_status()\n        completion = response.json()\n        return completion[\"choices\"][0][\"message\"][\"content\"]\n    except httpx.HTTPStatusError as e:\n        # this is triggered by raise_for_status and gives me access to resopnse body\n        #   i.e. useful with ollama when I request invalid model, the body explains that where as w/o the body its a generic 404 which is frustrating at best\n        print(f\"HTTP error occurred: {e}\")\n        print(f\"Response body: {e.response.text}\")  # response body\n    except Exception as e:\n        print(f\"{e}\")\n        return None\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/suggest.py:8-27\n\n\n\ndef get_openai_suggestion(passed_context: str, system_message: str, use: Service, max_tokens: int):\n    http_client = httpx.Client()\n    try:\n        body = {\n            \"model\": use.model,\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": system_message\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"{passed_context}\"\n                },\n            ],\n            \"max_tokens\": max_tokens,\n            \"n\": 1  # default\n\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/link.py:0-19\nfrom openai import OpenAI\nimport keyring\nimport sys\n\n#! todo refactor with other impl later as needed (composition most likely)\n\ndef generate_command(context: str):\n\n    service_name = 'openai'\n    account_name = 'ask'\n    password = keyring.get_password(service_name, account_name)\n    # windows => open Credential Manager => Windows Credentials tab => Generic Credentials section (add new)...  service_name => Internet/NetworkAddress, account_name => username\n    # macos => open Keychain Access => kind=app password, (security add-generic-password IIRC)\n\n    if password is None:\n        print(f\"No password found for {account_name} in {service_name}\")\n        sys.exit(1)\n\n    client = OpenAI(api_key=password)\n    try:\n\n",
        "_logged":true
      },{
        "role":"user",
        "content":"make all cases work like the \"--build21\" in ask_service case now.. not equality but `in`\n\nHere is the code I selected:\n```zsh/universals/3-last/ask-openai/services.py:276-297\n    if ask_service == \"--ollama\":\n        return use_ollama(model)\n    if ask_service == \"--groq\":\n        return use_groq(model)\n    if ask_service == \"--deepseek\":\n        return use_deepseek(model)\n    if ask_service == \"--vllm\":\n        return use_vllm(model)\n    if ask_service == \"--lmstudio\":\n        return use_lmstudio(model)\n    if ask_service == \"--anthropic\":\n        return use_anthropic(model)\n    if ask_service == \"--gh-copilot\":\n        return use_gh_copilot(model)\n    if \"--build21\" in ask_service:\n        return use_build21(model)\n    if ask_service == \"--inception\":\n        return use_inception(model)\n    if ask_service == \"--xai\":\n        return use_xai(model)\n    if ask_service == \"--openai\":\n        return use_openai(model)\n```\n\nThis is not necessarily a complete selection of nearby code, this is just the part I want help with. Preserve indentation!",
        "_logged":true
      }],
    "temperature":1,
    "top_p":1,
    "max_tokens":4096
  },
  "last_sse":{
    "timings":{
      "prompt_n":1078,
      "prompt_ms":401.432,
      "prompt_per_token_ms":0.37238589981447,
      "cache_n":383,
      "predicted_per_token_ms":4.3124478764479,
      "predicted_per_second":231.88686069956,
      "predicted_n":259,
      "prompt_per_second":2685.3863169852,
      "predicted_ms":1116.924
    },
    "model":"ggml-org/gpt-oss-120b-GGUF",
    "id":"chatcmpl-rsSrK1ghTRoNr5adVLYZQhgGBFbPOO8g",
    "object":"chat.completion.chunk",
    "system_fingerprint":"b8115-77d6ae4ac",
    "created":1772047699,
    "choices":[{
        "finish_reason":"stop",
        "index":0,
        "delta":[]
      }]
  }
}