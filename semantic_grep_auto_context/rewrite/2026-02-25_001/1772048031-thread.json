{
  "last_sse":{
    "choices":[{
        "index":0,
        "finish_reason":"stop",
        "delta":[]
      }],
    "system_fingerprint":"b8115-77d6ae4ac",
    "id":"chatcmpl-Y1esGvlnW4IDIWzAigG9By1qr4lxijab",
    "object":"chat.completion.chunk",
    "created":1772048033,
    "timings":{
      "predicted_n":246,
      "prompt_per_second":2982.7433940506,
      "prompt_n":1226,
      "predicted_per_second":231.6578319652,
      "cache_n":380,
      "predicted_per_token_ms":4.3167113821138,
      "prompt_ms":411.031,
      "prompt_per_token_ms":0.33526182707993,
      "predicted_ms":1061.911
    },
    "model":"ggml-org/gpt-oss-120b-GGUF"
  },
  "response_message":{
    "_logged":true,
    "finish_reason":"stop",
    "content":"```python\nif args.groq:\n    return use_groq(args.model)\nif args.lmstudio:\n    return use_lmstudio(args.model)\nif args.ollama:\n    return use_ollama(args.model)\nif args.deepseek:\n    return use_deepseek(args.model)\nif args.vllm:\n    return use_vllm(args.model)\nif args.anthropic:\n    return use_anthropic(args.model)\nif args.gh_copilot:\n    return use_gh_copilot(args.model)\nif args.build21:\n    return use_build21(args.model)\nif args.inception:\n    return use_inception(args.model)\nif args.xai:\n    return use_xai(args.model)\nif args.openai:\n    return use_openai(args.model)\n\nraise Exception(\"invalid service: \" + str(args))\n```",
    "reasoning_content":"User wants to \"return early\". Likely they want to simplify: if any flag set, return use, else raise. Could early return inside each branch. Already returns at end. Maybe they want to return early after setting use, not continue checking others. So add return after each assignment. Implement early returns.",
    "role":"assistant"
  },
  "request_body":{
    "temperature":1,
    "messages":[{
        "_logged":true,
        "role":"system",
        "content":"## Ground rules:\n- Follow the user's instructions. \n- DO NOT wrap answers in markdown code blocks, which means no triple backticks like ``` nor single backtick like ` .\n- Do not explain answers, just give me code. \n- If changing existing code, preserve unrelated code and comments. \n- Never add comments to the end of a line. \n- Never add stupid comments.\n- Be considerate with indentation. \n- Prefer readable code over of comments. \n- Prefer meaningful names for variables, functions, etc. Avoid ambiguous names."
      },{
        "_logged":true,
        "role":"user",
        "content":"## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n"
      },{
        "_logged":true,
        "role":"user",
        "content":"## Python Code Preferences\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer `list[]` over `List[]` to mimimize imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n"
      },{
        "_logged":true,
        "role":"user",
        "content":"# Semantic Grep matches: 5\n\nThis is automatic context based on my request. These may or may not be relevant.\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/single.py:0-7\nimport sys\nfrom services import args_to_use\nfrom suggest import generate\n\nsystem_message = \"You are a command line expert. Respond with a single, valid, complete command line. I intend to execute it. No explanation. No markdown. No markdown with backticks ` nor ```\"\nmax_tokens = 200\n\n\n\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/devtools.py:22-33\ndef main():\n\n    use = args_to_use()\n\n    stdin_context = sys.stdin.read()\n\n    command = get_openai_suggestion(stdin_context, system_message, use, max_tokens)\n    if command is None:\n        sys.exit(1)\n\n    # response is piped to STDOUT => STDIN of SHELL => command line buffer (as if user typed it)\n    print(command)\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/suggest.py:0-3\nfrom services import Service\nimport httpx  # import alone is ~58ms\n\n\n\n\n## ~/repos/github/g0t4/dotfiles/zsh/universals/3-last/ask-openai/suggest.py:4-8\ndef generate(passed_context: str, system_message: str, use: Service, max_tokens: int):\n    if use.name == \"anthropic\":\n        return get_anthropic_suggestion(passed_context, system_message, use, max_tokens)\n    else:\n        return get_openai_suggestion(passed_context, system_message, use, max_tokens)\n\n## ~/repos/github/g0t4/dotfiles/iterm2/scripts/AutoLaunch/wes/wes/asyncs.py:5-48\nasync def ask_openai_async_type_response(session, messages):\n\n    use = get_selected_service()\n    log(f\"using: {use}\")\n    api_key = use.api_key or \"\"  # must set empty at least\n    model = ChatOpenAI(model=use.model, api_key=api_key, base_url=use.base_url)\n    # FYI I had use.chat_completions_path too but shouldn't need it beyond MAYBE vllm (anthropic will be handled above w/ diff Chat client)\n\n    if use.name == \"anthropic\":\n        from langchain_anthropic import ChatAnthropic\n        model = ChatAnthropic(model_name=use.model, api_key=use.api_key, timeout=None, stop=None)\n        return\n\n    # max_tokens=use.max_tokens or 200,\n    # TODO temperature? and other model params on Service? (maybe rename it to be ServiceModel combo?)\n    chunks = model.stream(messages)\n    first_chunk = True\n    for chunk in chunks:\n        try:\n            # FYI no reasoning content for llama-server, s/b fine as I don't use that => if I want it, I can add in my ChatLlamaServer impl\n            # strip new lines to avoid submitting commands prematurely, is there an alternate char I could use to split the lines still w/o submitting (would have to check what the shell supports, if anything is possible)... one downside to not being a part of the line editor.. unless there is a workaround? not that I care much b/c multi line commands are not often necessary...\n            sanitized = chunk.content.replace(\"\\n\", \" \")  # PRN check if str before calling replace (i.e. can be list[str] or list[dict]... when is that the case and do I ever use it?)\n            if sanitized.strip() == \"\":\n                continue\n\n            if first_chunk:\n                log(f\"first_chunk: {sanitized}\")\n                sanitized = re.sub(r'```', '', sanitized).lstrip()\n                log(f\"sanitized: {sanitized}\")\n                log(f\"sanitized hex: {sanitized.encode('utf-8').hex()}\")\n                first_chunk = sanitized == \"\"  # stay in \"first_chunk\" mode until first non-empty chunk\n                await session.async_send_text(sanitized)\n            else:\n                await session.async_send_text(sanitized)\n\n            # TODO can I check finish_reason to see if it ran out of tokens?\n            # if choice0.finish_reason == \"length\":\n            #     await session.async_send_text(f\"ran out off tokens, increase max_tokens...\")\n            #     break\n\n        except Exception as e:\n            log(f\"Error processing chunk: {e}\\n chunk: {chunk}\")\n            await session.async_send_text(f\"Error processing chunk: {e}\")\n            return\n"
      },{
        "_logged":true,
        "role":"user",
        "content":"return early\n\nHere is the code I selected:\n```zsh/universals/3-last/ask-openai/services.py:219-245\n    if args.groq:\n        use = use_groq(args.model)\n    elif args.lmstudio:\n        use = use_lmstudio(args.model)\n    elif args.ollama:\n        use = use_ollama(args.model)\n    elif args.deepseek:\n        use = use_deepseek(args.model)\n    elif args.vllm:\n        use = use_vllm(args.model)\n    elif args.anthropic:\n        use = use_anthropic(args.model)\n    elif args.gh_copilot:\n        use = use_gh_copilot(args.model)\n    elif args.build21:\n        use = use_build21(args.model)\n    elif args.inception:\n        use = use_inception(args.model)\n    elif args.xai:\n        use = use_xai(args.model)\n    elif args.openai:\n        use = use_openai(args.model)\n    else:\n        raise Exception(\"invalid service: \" + str(args))\n\n    return use\n\n```\n\nThis is not necessarily a complete selection of nearby code, this is just the part I want help with. Preserve indentation!"
      }],
    "stream":true,
    "model":"",
    "chat_template_kwargs":{
      "reasoning_effort":"low"
    },
    "top_p":1,
    "max_tokens":4096
  }
}